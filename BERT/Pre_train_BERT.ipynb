{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-Training BERT from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\gloun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\gloun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\gloun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\gloun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\gloun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\gloun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\gloun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\gloun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\gloun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\gloun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wandb\n",
      "  Downloading wandb-0.19.6-py3-none-win_amd64.whl.metadata (10 kB)\n",
      "Collecting click!=8.0.0,>=7.1 (from wandb)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Downloading GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\gloun\\appdata\\roaming\\python\\python312\\site-packages (from wandb) (4.3.6)\n",
      "Collecting protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 (from wandb)\n",
      "  Downloading protobuf-5.29.3-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\gloun\\appdata\\roaming\\python\\python312\\site-packages (from wandb) (7.0.0)\n",
      "Collecting pydantic<3,>=2.6 (from wandb)\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting pyyaml (from wandb)\n",
      "  Downloading PyYAML-6.0.2-cp312-cp312-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\gloun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from wandb) (2.32.3)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-2.22.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting setproctitle (from wandb)\n",
      "  Downloading setproctitle-1.3.4-cp312-cp312-win_amd64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\gloun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from wandb) (75.8.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\gloun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click!=8.0.0,>=7.1->wandb) (0.4.6)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\gloun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=2.6->wandb)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic<3,>=2.6->wandb)\n",
      "  Downloading pydantic_core-2.27.2-cp312-cp312-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\gloun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=2.6->wandb) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gloun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gloun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gloun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gloun\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2024.7.4)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Downloading wandb-0.19.6-py3-none-win_amd64.whl (20.2 MB)\n",
      "   ---------------------------------------- 0.0/20.2 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 3.4/20.2 MB 16.7 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 9.2/20.2 MB 22.0 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 14.2/20.2 MB 22.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 18.9/20.2 MB 22.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 20.2/20.2 MB 20.6 MB/s eta 0:00:00\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Downloading GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "Downloading protobuf-5.29.3-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Downloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "Downloading pydantic_core-2.27.2-cp312-cp312-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 15.8 MB/s eta 0:00:00\n",
      "Downloading sentry_sdk-2.22.0-py2.py3-none-any.whl (325 kB)\n",
      "Downloading PyYAML-6.0.2-cp312-cp312-win_amd64.whl (156 kB)\n",
      "Downloading setproctitle-1.3.4-cp312-cp312-win_amd64.whl (12 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: smmap, setproctitle, sentry-sdk, pyyaml, pydantic-core, protobuf, docker-pycreds, click, annotated-types, pydantic, gitdb, gitpython, wandb\n",
      "Successfully installed annotated-types-0.7.0 click-8.1.8 docker-pycreds-0.4.0 gitdb-4.0.12 gitpython-3.1.44 protobuf-5.29.3 pydantic-2.10.6 pydantic-core-2.27.2 pyyaml-6.0.2 sentry-sdk-2.22.0 setproctitle-1.3.4 smmap-5.0.2 wandb-0.19.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install torch\n",
    "%pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gloun\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wandb'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm, trange\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m     31\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlogin(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1dedc34f4e12b8e0d723caafd0a0b53a0205dbb7\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wandb'"
     ]
    }
   ],
   "source": [
    "#basic imports\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import logging\n",
    "\n",
    "# Bring in PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Most of the examples have typing on the signatures for readability\n",
    "from typing import Optional, Callable, List, Tuple\n",
    "from copy import deepcopy\n",
    "\n",
    "# For data loading\n",
    "from torch.utils.data import Dataset, IterableDataset, TensorDataset, DataLoader\n",
    "import json\n",
    "import glob\n",
    "import gzip\n",
    "import bz2\n",
    "\n",
    "#import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For progress and timing\n",
    "from tqdm.auto import tqdm, trange\n",
    "import time\n",
    "import wandb\n",
    "import torch.nn.functional as F\n",
    "wandb.login(key='1dedc34f4e12b8e0d723caafd0a0b53a0205dbb7')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Tokenization\n",
    "\n",
    "To start with, we'll load in one of the tokenizers for the original BERT from the `tokenizers` package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "#                     TODO: YOUR CODE HERE                     #\n",
    "#\n",
    "#  1. Create a new BertWordPieceTokenizer using the specified vocab.txt file in the homework. \n",
    "#     Make sure that all tokens are lowed cased.\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "tokenizer = BertWordPieceTokenizer(\"text\\vocab.txt\")\n",
    "\n",
    "################################################################\n",
    "\n",
    "# Test the tokenizer\n",
    "print(tokenizer.encode(\"Hello, I am learning to tokenize\").tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.encode(\"I really wished my model would work.\").tokens)\n",
    "print(tokenizer.encode(\"Would you please put the pizza on the table?\").tokens)\n",
    "print(tokenizer.encode(\"Wow your dress looks incrdible!\").tokens)\n",
    "print(tokenizer.encode(\"I can't wait to drive home tonight on my motorcycle.\").tokens)\n",
    "print(tokenizer.encode(\"GRRRRAAAHHHH This is so frustrating...\").tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Building a Transformer Encoder and BERT\n",
    "\n",
    "The BERT model is a pre-trained transformer network. Creating a small BERT model will take two big pieces (1) building the transformer itself and (2) writing the code that incorporates the tranformer into BERT and has it set up for training. We'll try to simplify building this by thinking of the different pieces as building blocks that we can put together. Remember, all neural networks are _functions_ and you can compose functions together to make a new function. \n",
    "\n",
    "Let's take a look at the overall diagram for Transformers/BERT:\n",
    "![The diagram of the transformer newtork](https://devopedia.org/images/article/235/5113.1573652896.png)\n",
    "The trickiest part is the left where we need to deal with the scaled dot-product attention. You've already seen attention though in Homework 2, so some of this should be familiar. \n",
    "\n",
    "You'll implement the following pieces to put it all together.\n",
    "\n",
    "Steps:\n",
    "1. Embedding: BERT will learn word embeddings that are similar to word2vec's _but_ also incorporate the position of the embedding in the sequence\n",
    "2. Multi-headed Attention: The core part of the network that learns how much each token should pay attention to all other tokens\n",
    "3. A Feed-forward Layer: The layer that transformers the attention-combined representations\n",
    "4. The Transformer Encoder: The unified transformer network that combines attention with the feed-forward layer\n",
    "5. A BERT Classification Layer: The classificiation part of BERT\n",
    "6. A BERT Masked Language Modeling (MLM) Layer: The part of BERT that deals with MLM training\n",
    "7. The overall BERT model: The final BERT model architecture that supports both MLM and Classification\n",
    "\n",
    "Feel free to read the instructions for all steps in Part 2 before getting started to see how the pieces might fit together. To get everything working, you'll need all parts, which build on each other, so we recommend starting with the first and moving on from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.1: Embedding Layer\n",
    "BERT's input embeddings are normally the sum of three embeddings:\n",
    "- Token Embeddings: The input token embeddings\n",
    "- Position Embeddings: The position of the token in the sequence\n",
    "- Token Type Embeddings: The segment (sentence) the token belongs to\n",
    "\n",
    "The second piece helps BERT learn to distinguish that the same token is in different positions. Remember the attention mechanism works independently of where each of the tokens are; without positional information added to the word embedding, the model can't distinguish between words in different orders!\n",
    "\n",
    "The third piece was designed for the Next Sentence Prediciton (NSP) task during pre-training. Here, two sentences are provided as input with the special `[SEP]` token between them. The NSP task is a classifiction task based on whether the two sentences did or did not actually follow each other. Just like in word2vec, we would sample random sentences as not-next. The hope for this pretraining task was that it would help BERT learn discourse coherence. However, some later works have shown NSP doesn't actually help that much and training time is probably better spent on doing more MLM, so some more advanced models dropped this.\n",
    "\n",
    "For simplicity, in Homework 3, you only need to deal with token embeddings and positional embeddings, but do not need to deal with token-type embeddings.\n",
    "\n",
    "**NOTE:** When talking about embeddings, people (and these instructions) will talk about various things being embedded, e.g., words, tokens, wordpieces, subwords, etc. In practice, these are all based on whatever the tokenizer is producing, and BERT (and you) is agnostic to what is actually being embedded. When the instructions talk about \"token embeddings\" this are still just the output of the BERT WordPiece tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_dim: int, \n",
    "                 hidden_dim: int = 768, \n",
    "                 padding_idx: int = 0, \n",
    "                 max_seq_length: int = 512):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        '''\n",
    "        Initialize the Embedding Layers\n",
    "        '''\n",
    "\n",
    "        ################################################################################\n",
    "        #                             TODO: YOUR CODE HERE                             #\n",
    "        #                                                                              #\n",
    "        #  1. Create two Embedding objects for the words and the positions.            #\n",
    "        #     For the word embeddings keep track of which index is the padding index.  #        \n",
    "        # \n",
    "        self.word_embedding = nn.Embedding(vocab_dim, hidden_dim, padding_idx=padding_idx)\n",
    "        self.position_embedding = nn.Embedding(max_seq_length, hidden_dim)                                                                             #\n",
    "        ################################################################################\n",
    "\n",
    "    def forward(self, token_ids: torch.Tensor, \n",
    "                ) -> torch.Tensor:\n",
    "        \n",
    "        '''\n",
    "        Define the forward pass of the Embedding Layers\n",
    "        '''\n",
    "        \n",
    "        ############################################################################\n",
    "        #                               TODO: YOUR CODE HERE                       #\n",
    "        #                                                                          #\n",
    "        # 1. Look up the relevant token embeddings from the word_embeddings layer  #\n",
    "        words = self.word_embedding(token_ids)\n",
    "        seq_ids = torch.arange(token_ids.size(1), dtype = torch.long, device=token_ids.device)\n",
    "        seq_ids = seq_ids.unsqueeze(0).expand_as(token_ids)\n",
    "        pos = self.position_embedding(seq_ids)\n",
    "        # 2. Return the sum of the token embeddings and the positional embeddings  #\n",
    "        return words + pos\n",
    "        #                                                                          #\n",
    "        ############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.2: Multi-Head Attention\n",
    "\n",
    "This is the trickiest part of the homework where we implement the attention part of the transformer. Let's take a look at the attention:\n",
    "\n",
    "![The attention network](https://www.tutorialexample.com/wp-content/uploads/2021/03/The-structure-of-Multi-Head-Attention.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional TODO: Try implementing attention with some embeddings to get a sense of the core parts\n",
    "# Note that this is not a complete implementation of the attention mechanism in the transformer,\n",
    "# which also includes some scaling and masking operations as well as dealing with multiple heads.\n",
    "\n",
    "# Generate some Q, K, V embeddings for a batch of 10 sequences of length 4 with embedding size 7\n",
    "q_emb = torch.randn(10, 4, 7)\n",
    "k_emb = torch.randn(10, 4, 7)\n",
    "v_emb = torch.randn(10, 4, 7)\n",
    "\n",
    "# Start by computing the dot product of the Q and K embeddings\n",
    "dot_prod = (q_emb @ k_emb.transpose(-1, -2)) \n",
    "\n",
    "# Then apply a softmax to get the attention weights\n",
    "attn = nn.functional.softmax(dot_prod, dim=-1)\n",
    "\n",
    "# This should be torch.Size([10, 4, 4]) --- i.e., how much each word (4 words) pays attention to each other word\n",
    "print(attn.shape)\n",
    "\n",
    "# Now compute the weighted words by multiplying the attention weights by the V embeddings\n",
    "weighted_output = attn @ v_emb\n",
    "\n",
    "# This should be torch.Size([10, 4, 7]) --- i.e., the same shape as our inputs, but the embedings are weighted combinations of the V embeddings!\n",
    "print(weighted_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional TODO: How to implicitly represent attention weights in a single layer\n",
    "\n",
    "# Let's start with a single projection of the input embedding for Q for a sequence of 3 words with embedding size 8\n",
    "# NOTE: we'll use randint here to make it easier to see the reshaping when we print the tensors\n",
    "q_emb = torch.randint(10, (3, 8))\n",
    "print(q_emb)\n",
    "# If we have two attention heads we can reshape this tensor so that the first dimension is the number of heads\n",
    "q_s = q_emb.shape\n",
    "# Should be torch.Size([3, 8])\n",
    "print(q_s)\n",
    "n_heads = 2\n",
    "multihead_q_emb = q_emb.view(n_heads, q_s[0], q_s[1]//n_heads)\n",
    "\n",
    "# Should be torch.Size([2, 3, 4])\n",
    "print(multihead_q_emb.shape)\n",
    "\n",
    "# Check that the reshaping is correct by looking at which values when where\n",
    "print(multihead_q_emb)\n",
    "\n",
    "# Remember: Each head has its own attention! So we'll need to use this kind of reshaping for K and V so we can evantually compute\n",
    "# the per-head specifici attention weights. \n",
    "\n",
    "# Optional TODO: try creating the K projection and re-shaping it and then the calculate its attention weights using the logic from the above cell\n",
    "# Note that you'll now have another dimension in the tensor! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, hidden_size: int, num_heads: int):\n",
    "        '''\n",
    "        Arguments:\n",
    "        hidden_size: The total size of the hidden layer (across all heads)\n",
    "        num_heads: The number of attention heads to use\n",
    "        '''\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        '''\n",
    "        Initialize the Multi-Headed Attention Layer\n",
    "        '''\n",
    "\n",
    "        ###########################################################################################################################\n",
    "        #                                                     TODO: YOUR CODE HERE                                                #\n",
    "        #\n",
    "        self.num_heads = num_heads\n",
    "        # 1. Figure out how many dimensions each head should have      \n",
    "        self.head_dim = hidden_size // num_heads\n",
    "        # 2. Create linear layers to turn the input embeddings into the query, key, and value projections  \n",
    "        self.K = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
    "        self.V = nn.Linear(hidden_size, hidden_size)\n",
    "        # 3. Calculate the scale factor (1 / sqrt(per-head embedding size))\n",
    "        self.scale = 1/ np.sqrt(self.head_dim)                                                       \n",
    "        #                                                                                                                         #                                            \n",
    "        # NOTE: Each of the Q, K, V projections represents the projections of *each* of the heads as one long sequence.           #\n",
    "        #       Each of the layers is implicitly representing each head in different parts of its dimensions. E.g., if you        #\n",
    "        #       have 4 heads and 16 dimensions, the first 4 dimensions are the first head, the second 4 dimensions are            #\n",
    "        #       the second head, etc.                                                                                             #                        \n",
    "        ###########################################################################################################################\n",
    "\n",
    "    def forward(self, embeds: torch.Tensor, \n",
    "                mask: Optional[torch.Tensor] = None\n",
    "                ) -> torch.Tensor:\n",
    "        '''\n",
    "        Arguments:\n",
    "        embeds: The input embeddings to compute the attention over\n",
    "        mask: A boolean mask of which tokens are valid to use for computing attention (see collate below)\n",
    "        '''\n",
    "\n",
    "        #####################################################################################################################################################\n",
    "        #                                                   TODO: YOUR CODE HERE                                                                            #\n",
    "        #                                                                                                                                                   #\n",
    "        # This is the hard part of the assignment where you'll need to implement the multi-headed attention mechanism.                                      #\n",
    "        # The cell above has the core logic for the attention mechanism to get you started with getting the tensor                                          #\n",
    "        # shapes lined up correctly. We recommend working through that manually with some small examples to get a sense                                     #\n",
    "        # of what is happening at each step.                                                                                                                #\n",
    "        #                                                                                                                                                   #\n",
    "        # One of the key parts to work through is how to handle the shapes of the different embeddings. For attention to work                               #\n",
    "        # and be efficient, we'll only need to do a few matrix multiplications and a softmax. The key is to figure out how to                               #\n",
    "        # do this by getting the tensors into the right shapes. We strongly recommend trying to write comments at each step                                 #\n",
    "        # That describe the shape of the tensors at each step in terms of what each representings.   This will help you understand                          #\n",
    "        # what is happening and debug.                                                                                                                      #\n",
    "        #                                                                                                                                                   #\n",
    "        # We recommend using notation like the following which you'll also see in papers and blogs:                                                         #\n",
    "        #  - B: Batch size                                                                                                                                  #\n",
    "        #  - H: Number of heads                                                                                                                             #\n",
    "        #  - T: Sequence length                                                                                                                             #\n",
    "        #  - D: Embedding size                                                                                                                              #\n",
    "        #                                                                                                                                                   #\n",
    "        # 1. Figure out what are the dimensions of the input embeddings and which dimensions                                                                #\n",
    "        #    represent what (e.g., the batch size, sequence length, etc.)\n",
    "        B = embeds.shape[0]\n",
    "        #H = self.num_heads\n",
    "        T = embeds.shape[1]\n",
    "        D = embeds.shape[2] \n",
    "        # 2. Project the input embeddings into the Q, K, and V spaces\n",
    "        Q = self.Q(embeds).view(B, self.num_heads, T, D//self.num_heads)\n",
    "        K = self.K(embeds).view(B, self.num_heads, T, D//self.num_heads)\n",
    "        V = self.V(embeds).view(B, self.num_heads, T, D//self.num_heads)\n",
    "\n",
    "        # 3. Compute the attention weights from the Q and K projections (be sure to scale the dot product by the scale factor!)\n",
    "        dot_prod = self.scale*(Q @ K.transpose(-2, -1)) \n",
    "        # 4. *If their is a mask*, apply the mask to the attention weights where masked values are set to -inf  \n",
    "        if mask != None:\n",
    "            expanded_mask = mask.unsqueeze(1).unsqueeze(1)  # Shape: (batch size, 1, sequence length, 1)\n",
    "            #mask = expanded_mask.expand_as(attn)  # Shape: (batch size, number of heads, sequence length, sequence length)  \n",
    "            dot_prod = dot_prod.masked_fill(expanded_mask == 0, float('-inf'))\n",
    "        attn = F.softmax(dot_prod, dim=-1)\n",
    "        # 5. Compute the weighted sum of the V embeddings using the attention weights    \n",
    "        attention = attn @ V\n",
    "        # 6. Return the re-weighted output values *and* the attention weights in the shape (Batch, Heads, SeqLen, SeqLen)                                   #\n",
    "        #    We'll use the attention weights for visualization later                                                                                        #\n",
    "        return attention.view(B, -1, D), attn.view(B, self.num_heads, T, T)                                                                                                                                               #\n",
    "        # NOTE: when we say dimension, we're referring to how many axes are in a tensor. E.g., a vector                                                     #\n",
    "        #       is a one-dimensional tensor, a matrix is a two-dimensional tensor, etc. Each dimension has a size too (number of components),               #\n",
    "        #       e.g., a 3x4 tensor has 2 dimensions, where the first has 3 elements/components and the second has 4 elements/components.                    #\n",
    "        # NOTE: You will probably want to use the .view() method to reshape the input embeddings with respect to the number of heads                        #\n",
    "        #       so that you can get a tensor where one dimension corresponds to each head                                                                   #\n",
    "        # NOTE: You may want to use the .transpose() method to swap dimensions (e.g., to move the heads dimension to the front)                             #\n",
    "        # NOTE: Check out masked_fill for applying the mask to the attention weights                                                                        #\n",
    "        # NOTE: You will not need to concatenate anything in practice because the Q, K, V projections already represent concatenated head-specific values   #\n",
    "        #    in the right dimensions.                                                                                                                       #\n",
    "        # HINT: You can reshape any tensor with view to \"add a dimension\" to it.                                                                            #\n",
    "        #    E.g., if you have a tensor with shape (B, T, D) and you want to add a dimension to the front, you can do tensor.view(B, 1, T, D)               #\n",
    "        #####################################################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.3 Define the Feed-Forward Layer\n",
    "\n",
    "The feed forward layer is a simple two-layer feed forward network (FFN) with an activation function between the layers. This network usually follows the multi-headed attention output that allows its content representation to be tranformed and aggregated. Below, we'll make a function that return this network using `nn.Sequential` which takes in a tuple or list of layers and activation functions where any input is passed through each function in order and then the output is returned.\n",
    "\n",
    "In a tranfsormer, typically the FFN is wider than the embedding size, usally with an expansio factor of 4. This increased number of neurons allows the model to capture more interactions between dimensions of the embedding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward_layer(\n",
    "    hidden_size: int, \n",
    "    feed_forward_size: Optional[int] = None, \n",
    "    activation: nn.Module = nn.GELU()\n",
    "):\n",
    "    '''\n",
    "    Arguments:\n",
    "      - hidden_size: The size of the input and output of the feed forward layer. \n",
    "      - feed_forward_size: The size of the hidden layer in the feed forward network. If None, defaults to 4 * hidden_size. This size\n",
    "        specifies the size of the middle layer in the feed forward network.\n",
    "      - activation: The activation function to use in the feed forward network\n",
    "\n",
    "    Returns: \n",
    "    '''\n",
    "    ################################################################\n",
    "    #                     TODO: YOUR CODE HERE                     #\n",
    "    # Implement the feed forward layer as described in the slides  #\n",
    "    # The feed forward layer is a simple three-layer neural network#\n",
    "    # with an activation function.                                 #\n",
    "    if feed_forward_size == None: feed_forward_size = 4*hidden_size\n",
    "    return nn.Sequential(nn.Linear(hidden_size, feed_forward_size), activation, nn.Linear(feed_forward_size, hidden_size))\n",
    "    # NOTE: It maps from hidden_size to feed_forward_size and then #\n",
    "    #       back to hidden_size.                                   #\n",
    "    ################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.4 Building a Transformer Block as an Encoder Layer\n",
    "\n",
    "Let's finish putting together our tranformer pieces into a single network that (1) computes the self-attention to get contextualized word representations and (2) passes those representations through our feed-forward neural network layers. \n",
    "\n",
    "During training, we'll also add some probability of using [dropout](https://machinelearningmastery.com/using-dropout-regularization-in-pytorch-models/) which is also implemented in pytorch. Dropout will randomly zero-out some values when training so the model learns to be robust to the value of any one neuron. In practice, when you switch between `train()` and `eval()`, under the hood, this will turn on/off things like dropout automatically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int = 256, # NOTE: normally 768, but keep it small for homework\n",
    "        num_heads: int = 12,\n",
    "        dropout: float = 0.1,\n",
    "        activation: nn.Module = nn.GELU(),\n",
    "        feed_forward_size: Optional[int] = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        ################################################################\n",
    "        #                     TODO: YOUR CODE HERE                     #\n",
    "        # Now we can put it all together to create one layer of the    #\n",
    "        # transformer encoder.                                         #\n",
    "        #                                                              #\n",
    "        # 1. Create a MultiHeadedAttention layer with the specified    #\n",
    "        #    number of heads.                                          #\n",
    "        self.mha = MultiHeadedAttention(hidden_size, num_heads)\n",
    "        # 2. Create a feed forward layer with the specified activation #\n",
    "        #    function.                                                 #\n",
    "        self.ffl = feed_forward_layer(hidden_size, feed_forward_size, activation)\n",
    "        # 3. Save the hidden_size, dropout, and feed_forward_size      #\n",
    "        #    as attributes.                                            #\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = dropout\n",
    "        self.feed_forward_size = feed_forward_size\n",
    "        \n",
    "        # 4. Define a forward method that takes in an input tensor     #\n",
    "        #    and an optional mask tensor and returns the output tensor #\n",
    "        #    and the attention weights that go through the multi-      #\n",
    "        #    headed attention layer and the feed forward layer.        #\n",
    "        ################################################################\n",
    "\n",
    "    def maybe_dropout(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        ################################################################\n",
    "        #                     TODO: YOUR CODE HERE                     #\n",
    "        # Implement the dropout layer to be used in the forward pass   #\n",
    "        # of the transformer encoder layer (if dropout was specified)  #\n",
    "        if self.training and self.dropout > 0:\n",
    "            return F.dropout(x, p=self.dropout, training=True)\n",
    "        return x\n",
    "        ################################################################\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None):\n",
    "        '''\n",
    "        Returns the output of the transformer encoder layer and the attention weights from the self-attention layer\n",
    "        '''\n",
    "        \n",
    "        ################################################################\n",
    "        #                     TODO: YOUR CODE HERE                     #\n",
    "        #\n",
    "        # 1. Pass the input tensor through the self-attention layer \n",
    "        output, attn_weights = self.mha(x, mask)\n",
    "        # 2. Call maybe_dropout on the output of the self-attention\n",
    "        output = self.maybe_dropout(output) + x \n",
    "        # 3. Pass the output of the self-attention through the feed forward network\n",
    "        output = self.ffl(output) + output\n",
    "        # 4. return the output of the feed forward network and the attention weights\n",
    "        return output, attn_weights   \n",
    "        #\n",
    "        ################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.5: Masked Language Modeling Head\n",
    "\n",
    "A BERT model usually comes with multiple \"heads\" that are used for different tasks. One of these heads is used for the masked language model (MLM) task during pretraining. \n",
    "        \n",
    "In practice, a \"head\" is just a linear layer that maps the hidden size to the output size. This linear layer allows the model to adapt the token representations to a particular task, while keeping the core transformer model parameters the same across tasks. A model could have multiple heads for different tasks.  In our implementation, we'll have two heads, one for the masked language modeling task and a second for classification.\n",
    "\n",
    "The MLM head maps the contextualized token embedding to the vocabulary, so if we have some embedding $e_i$, we're learning a weight matrix $W_{mlm}$ of size $|e_i| \\times |V|$. In BERT pre-training, this weight matrix is said to be _tied_ to the input embeddings; in practice, that means we use the same weights from the `Embedding` (!), except this time the weights are used as a linear layer! (NOTE: This means you're not defining a separate linear layer). You might also see this called \"parameter sharing\" where the same parameters (i.e., weights) are used in different parts of the neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLMHead(nn.Module):\n",
    "    def __init__(self, word_embeddings: nn.Embedding):\n",
    "        '''\n",
    "        Arguments:\n",
    "            word_embeddings: The word embeddings to use for the prediction\n",
    "        '''\n",
    "\n",
    "        super().__init__()\n",
    "        self.word_embeddings = word_embeddings\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x: The input tensor to the MLM head containing a batch of sequences of\n",
    "           contextualized word embeddings (activations from the transformer encoder \n",
    "           layers)\n",
    "        '''\n",
    "        \n",
    "        ################################################################\n",
    "        #                 TODO: YOUR CODE HERE                         #\n",
    "        #                                                              #\n",
    "        # The MLM head is used to predict the original token from      #\n",
    "        # the masked token. The prediction is over the whole \n",
    "        # vocabulary so we'll need an activation. To make this work,\n",
    "        # we'll generate a tensor the length of the vocabulary size\n",
    "        # that we can push through a softmax to get the probabilities\n",
    "        # of each token being present.\n",
    "        return (x @ self.word_embeddings.weight.transpose(0,1))\n",
    "        #                                                              #\n",
    "        # NOTE: The head should not have an activation function.       #\n",
    "        # NOTE: The head should be tied to the input embeddings (i.e., #\n",
    "        #       the head should map the embeddings to the vocab size). #\n",
    "        #       In other words, the MLM head directly predicts the     #\n",
    "        #       token from the learned embeddings instead of the       #\n",
    "        #       last hidden states.                                    #\n",
    "        # HINT: You can get the tensor of the word embeddings to use   #\n",
    "        #       for prediction from the word_embeddings object.        #\n",
    "        # HINT: Desipte all this writing, this function is only a \n",
    "        #       single line of code.                                   #\n",
    "        ################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.6: Classification Pooler Head\n",
    "\n",
    "When we pre-train, we'll learn a special embedding called `[CLS]` that is the first embedding in any sequences and loosely approximates the overall meaning/semantics of the input text. Frequently, when we fine-tune a BERT classifier, we're using this `[CLS]` token as the summary of the input and updating the weights accordingly. \n",
    "\n",
    "Here, we'll create a network that will add a `Linear` layer on top of the `[CLS]` token which can be fine-tuned to do classification later. This linear layer allows use to adapt/project the `[CLS]` to a representation more suitable for classification.  This is the classification head of BERT.\n",
    "\n",
    "We'll use this network later when defining the `BERT` class below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooler(nn.Module):\n",
    "    def __init__(self, hidden_size: int = 768):\n",
    "        super().__init__()\n",
    "\n",
    "        ################################################################\n",
    "        #                     TODO: YOUR CODE HERE                     #\n",
    "        # The BERT model usually uses the first token of the sequence  #\n",
    "        # to represent the entire sequence (this is the [CLS] token).  #\n",
    "        #                                                              #\n",
    "        # The pooler layer is a simple linear layer that maps the      #\n",
    "        # representation of the [CLS] token to the hidden size.        #\n",
    "        self.linear = nn.Linear(hidden_size, hidden_size)\n",
    "        #                                                              #\n",
    "        # This pooled representation is used as the input to the       #\n",
    "        # classification layer defined in the later cell.              #\n",
    "        ################################################################\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        ################################################################\n",
    "        #                     TODO: YOUR CODE HERE                     #\n",
    "        #\n",
    "        # 1. Pass the [CLS] embedding through the dense layer \n",
    "        output = self.linear(x[:,0])\n",
    "        # 2. Pass the output of the dense layer through the activation\n",
    "        active = torch.sigmoid(output)\n",
    "        # 3. Return the output of the activation\n",
    "        return active\n",
    "        # OPTIONAL TODO: One other way you can represent the contents of \n",
    "        #             the entire sequence is to use the mean of all the non-special\n",
    "        #             token embeddings (also known as \"mean pooling\"). \n",
    "        #             You can try implementing that here and add a flag to the\n",
    "        #             Pooler __init__ function to switch between the two approaches.\n",
    "        #\n",
    "        ################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.7: The BERT network!\n",
    "\n",
    "Finally! We have arrived at putting all the pieces together into a single neural network. Our BERT model will have the main attention plus feed-forward network components and then two heads: one for MLM and one for Classification. At model creation time, we'll specify which \"mode\" the model should be in (MLM or classification).\n",
    "\n",
    "This implementation is a good reminder that neural networks (e.g., `nn.Module`) are just functions on inputs. We can compose and stack them together to get some really useful (and cool) outputs as a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        padding_idx: int = 0,\n",
    "        hidden_size: int = 768,\n",
    "        num_heads: int = 12,\n",
    "        num_layers: int = 12,\n",
    "        dropout: float = 0.1,\n",
    "        activation: nn.Module = nn.GELU(),\n",
    "        feed_forward_size: Optional[int] = None,\n",
    "        mode: str = \"mlm\",\n",
    "        num_classes: Optional[int] = None\n",
    "    ):\n",
    "        '''\n",
    "        Defines BERT model architecture. Note that the arguments are the same as the default\n",
    "        BERT model in HuggingFace but we'll be training a *much* smaller model for this homework.\n",
    "\n",
    "        Arguments:\n",
    "        vocab_size: The size of the vocabulary (determined by the tokenizer)\n",
    "        padding_idx: The index of the padding token in the vocabulary (defined by the tokenizer)\n",
    "        hidden_size: The size of the hidden layer and embeddings in the transformer encoder\n",
    "        num_heads: The number of attention heads to use in the transformer encoder\n",
    "        num_layers: The number of layers to use in the transformer encoder (each layer is a TransformerEncoderLayer)\n",
    "        dropout: The dropout rate to use in the transformer encoder (what % of times to randomly zero out activations)\n",
    "        activation: The activation function to use in the transformer encoder\n",
    "        feed_forward_size: The size of the hidden layer in the feed forward network in the transformer encoder. If None, defaults to 4 * hidden_size\n",
    "        mode: The mode of the BERT model. Either \"mlm\" for masked language modeling or \"classification\" for sequence classification\n",
    "        num_classes: The number of classes to use in the classification layer.\n",
    "        '''\n",
    "\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        ################################################################\n",
    "        #                     TODO: YOUR CODE HERE                     #\n",
    "        # Now we can put it all together to create the BERT model.     #\n",
    "        #                                                              #\n",
    "        # A BERT model is just a stack of transformer encoder layers   #\n",
    "        #                                                              #\n",
    "        # followed by a pooler layer and a classification layer.       #\n",
    "        # 1. Create a BertPositionalEmbedding layer with the specified #\n",
    "        #    vocab size, hidden size, and padding index.               #\n",
    "        self.bpe = BertPositionalEmbedding(vocab_size, hidden_size, padding_idx)\n",
    "        # 2. Create a stack of transformer encoder layers with the     #\n",
    "        #    specified number of layers, hidden size, number of heads, #\n",
    "        #    dropout, activation function, and feed forward size.      #\n",
    "        self.tel_layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(hidden_size, num_heads, dropout, activation, feed_forward_size) \n",
    "            for _ in range(num_layers)])\n",
    "        # 3. Create an MLMHead layer with the specified vocab size and #\n",
    "        #    padding index.                                            #\n",
    "        self.mlm = MLMHead(self.bpe.word_embedding)\n",
    "        # 4. Create a Pooler layer with the specified hidden size.     #\n",
    "        self.pooler = Pooler(hidden_size)\n",
    "        # 5. Create a classification layer with the specified number   #\n",
    "        #    of classes.                                               #\n",
    "        #\n",
    "        if num_classes != None:\n",
    "            self.linear = nn.Linear(hidden_size, num_classes)\n",
    "        self.mode = mode\n",
    "        # HINT: you can use nn.ModuleList to stack layers.  \n",
    "        #\n",
    "        ################################################################\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        x: torch.Tensor, \n",
    "        mask: Optional[torch.Tensor] = None, \n",
    "    ) -> torch.Tensor:\n",
    "        '''\n",
    "        arguments:\n",
    "        x: The input token ids\n",
    "        mask: The attention mask to apply to the input (see the collate function below)\n",
    "        '''\n",
    "        \n",
    "        ################################################################\n",
    "        #                     TODO: YOUR CODE HERE                     #\n",
    "        #\n",
    "        # 1. Get the embeddings for the input token ids\n",
    "        embeds = self.bpe(x)\n",
    "        # 2. Calculate the attention weights for each layer (be sure to save the attention weights to return)\n",
    "        weights = []\n",
    "        for layer in self.tel_layers:\n",
    "            embeds, attn_weights = layer(embeds, mask=mask)\n",
    "            weights.append(attn_weights)\n",
    "        # 3a. If the mode is \"mlm\", pass the embeddings through the MLM head and return the output\n",
    "        output = None\n",
    "        if self.mode == 'mlm':\n",
    "            output = self.mlm(embeds)\n",
    "        # 3b. If the mode is \"classification\", pass the embeddings through the classification head\n",
    "        elif self.mode == \"classification\":\n",
    "            pooled = self.pooler(embeds)\n",
    "            output = self.linear(pooled)\n",
    "        # 4. Return the output (from the relevant head) and the attention weights\n",
    "        return output, torch.stack(weights)\n",
    "        #\n",
    "        ################################################################\n",
    "\n",
    "    def init_layer_weights(self, module):\n",
    "        ################################################################\n",
    "        #                     TODO: YOUR CODE HERE                     #\n",
    "        #\n",
    "        #  Initialize the weights of the model with mean 0 and std 0.02\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        # \n",
    "        ################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We can verify that the model is working by running a quick test.\n",
    "'''\n",
    "\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "tokens = tokenizer.encode(sentence)\n",
    "# to tensor\n",
    "token_ids = torch.tensor(tokens.ids)\n",
    "bert = BERT(vocab_size=tokenizer.get_vocab_size(), \n",
    "            hidden_size=768, \n",
    "            num_heads=4, \n",
    "            num_layers=2,\n",
    "        )\n",
    "\n",
    "attention_mask = (token_ids != 0).float()\n",
    "\n",
    "output, attn = bert(token_ids.unsqueeze(0), attention_mask.unsqueeze(0))\n",
    "print(tokenizer.get_vocab_size())\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Training\n",
    "\n",
    "After all that implementation, it's time to pre-train our BERT model. We'll focus specifically on MLM training and then in a separately notebook you'll used the pre-trained BERT for Parts 5 and 6. \n",
    "\n",
    "To pre-train, we'll need to accomplish three pieces:\n",
    "1. First, we'll need to create a `Dataset` class that says how to load and process our text data for MLM training. \n",
    "2. Second, we'll need to create a `collate` function that tells the `DataLoader` how to combine multiple training examples from our dataset into a batch. The `collate` function is critical because not all of our input texts have the same size (different sequence lengths) which will create a wrinkle for giving a model a single `Tensor`\n",
    "3. Third, we'll write the core training loop.\n",
    "\n",
    "Each of the pieces below goes over more details. As you progress as a practitioner, you'll frequently need to write `Dataset` and `collate` functions for more bespoke kinds of training tasks. This part of the assignment is intended to show you how to do some simple implementations that you can re-use later. You'll likely reuse some/all of this code in Parts 5 and 6 when setting up these functions for classification!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3.1: Create A Dataloader for Masked Language Modeling\n",
    "\n",
    "A `Dataset` is kind of like a glorified `list` object in python. The Dataset class helps us to load and process the data, which providing functionality that lets us access the data\n",
    "\n",
    "The main function of the Dataset class is to get the number of samples and to get a sample from the dataset. The core functionality you'll need to implement in this part of the assignment is the following:\n",
    "- Load the data and tokenize it using the tokenizer.\n",
    "- The __len__ method to return the number of samples in the dataset.\n",
    "- The __getitem__ method to return a sample from the dataset.\n",
    "\n",
    "As a sidenote, the `Dataset` class provides a very important abstraction for training and more sophisticated `Dataset` implementations will do usfeul things like keep only some of the dataset in memory and proactively fetch data from desk to keep the overall memory footprint low. Others may do pre-processing on the fly to avoid having large `Tensor` objects in memory (e.g., loading images and preparing them for image-based learning). Just think of how large some datasets might get---as a practitioner, these implementations are critical for efficient training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLMDataset(Dataset):\n",
    "    def __init__(self, tokenizer, data: List[str], max_seq_length=128, mlm_probability=0.15):\n",
    "\n",
    "        ##################################################################################################################\n",
    "        # TODO: YOUR CODE HERE \n",
    "        # \n",
    "        # 1. Store the arguments as fields\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.mlm_probability = mlm_probability\n",
    "        self.tokens = []\n",
    "        #                                                                                                                #                                    #\n",
    "        ##################################################################################################################\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def tokenize(self):\n",
    "        '''\n",
    "        Tokenizes the text in self.data, performing any preprocessing and storing the tokenized data in a new list.\n",
    "        '''\n",
    "\n",
    "        ##################################################################################################################\n",
    "        # TODO: YOUR CODE HERE \n",
    "        #                                                                                                                #                                                     #\n",
    "        tokenized_data = []\n",
    "        for text in self.data:\n",
    "            # 1. Tokenize the data using the tokenizer.  \n",
    "            token_ids = self.tokenizer.encode(text).ids  \n",
    "            # 2. Truncate the sequence to the maximum sequence length.                                                       #\n",
    "            token_ids = token_ids[:self.max_seq_length]\n",
    "            # 3. Add the tokenized data to a list.  \n",
    "            tokenized_data.append(token_ids)\n",
    "        self.tokenized_data = deepcopy(tokenized_data)\n",
    "        return tokenized_data\n",
    "        # NOTE: To save memory, you can delete self.data after tokenizing since you'll have the copy of \n",
    "        #       the tokenized data (as ids) and won't need the raw text later.\n",
    "        del self.data\n",
    "        #\n",
    "        \n",
    "        ##################################################################################################################\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        Returns the list of the token ids of an instance in the dataset and a list of the labels for MLM (one label per token).\n",
    "        '''\n",
    "\n",
    "        ################################################################\n",
    "        #                     TODO: YOUR CODE HERE                     #\n",
    "        #\n",
    "        # 1. Get the tokenized data at the specified index\n",
    "        token_ids = deepcopy(self.tokenized_data[idx])\n",
    "        # 2. Create the mask (i.e., which words the model has to predict). Special tokens are never masked.\n",
    "        #    Non-masked tokens should be set to -100, which will be ignored in the loss function. \n",
    "        #    The masked tokens should be set to the original token ids. Use the specified masking probability.  \n",
    "        labels = []\n",
    "        for i, token_id in enumerate(token_ids):\n",
    "            #exclude special tokens \n",
    "            if token_id == 101 or token_id == 102 or token_id == 0:\n",
    "                labels.append(-100)\n",
    "            else: \n",
    "                if np.random.uniform() < self.mlm_probability:\n",
    "                    labels.append(token_id)\n",
    "                    token_ids[i] = tokenizer.token_to_id('[MASK]')\n",
    "                else:\n",
    "                    labels.append(-100)\n",
    "\n",
    "        return token_ids, labels     \n",
    "        # \n",
    "        # Hint: Use the tokenizer's functions to get IDs as needed\n",
    "        ################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Verify that the dataset is working by running a quick test.\n",
    "'''\n",
    "\n",
    "# create a fake dataset using 26 aphabets, 1000 sentences, 10-20 words per sentence randomly\n",
    "data = [' '.join([chr(97 + i) for i in range(random.randint(10, 20))]) for _ in range(1000)]\n",
    "\n",
    "dataset = MLMDataset(tokenizer, data)\n",
    "dataset.tokenize()\n",
    "\n",
    "# get the first item\n",
    "input_ids, labels = dataset[0]\n",
    "print('input_ids:', input_ids)\n",
    "print('labels:', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tet with one sentence\n",
    "data = [\"The quick brown fox jumps over the lazy dog.\",\n",
    "        \"The quick brown fox jumps over the lazy dog.\",\n",
    "        \"The quick brown fox jumps over the lazy dog.\",]\n",
    "dataset = MLMDataset(tokenizer, data)\n",
    "dataset.tokenize()\n",
    "print(dataset[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3.2: Create A `collate` function to prepare a batch for training\n",
    "\n",
    "Once the dataset is ready, we can use the `DataLoader` class  to load the data from the `Dataset` class, much like you did for Homework 2.  Remember that the Dataset class has the __getitem__ method  that returns the input_ids and the labels.\n",
    "    \n",
    "Like Homework 2, we'll want to train using a _batch_ of items. This is good for two reasons. First, batching helps us learn from multiple examples at the same time, so the gradient is a bit smoother. Batching provides a good trade-off between SGD (one item at a time) and full GD (all items at once). Second, and perhaps more importantly, batching allows us to maximize the throughput of our computing resources. Depending on the hardware, some matrix operations are the same speed for different sized matrices, so if we can get more examples used to trained per step, this reduces the overall number of steps. You may have seen this in Homework 2 where increasing the batch size dropped the training time, up to some point. \n",
    "\n",
    "By default, the `DataLoader` will randomly sample $b$ items from `Dataset` where $b$ is the batch size and turn those into a single `Tensor` to pass as input to the model. To create the batch itself, the `collate` function will tell the `DataLoader` how to turn multiple items into a single `Tensor`.\n",
    "\n",
    "However, we have a major wrinkle here. When we train BERT for MLM over sequences, not all the sequences have the same length. A batch itself is represented as a `Tensor`. When all the instances in the batch have the same length, we can concatenate/stack them. For example, if we had 10 instances of sequences of length 5, we can create a tensor that is size (10, 5). However, if some of those sequences have different lengths, we no longer can create a Tensor with a single length dimension! What to do?\n",
    "\n",
    "To solve this, we'll need to write the `collate` function so that it makes all the sequences have the same length. Typically this is done by adding a special `[PAD]` token to the sequences so that they all have the same number of tokens. However, this extra token is meaningless!  If we don't recognize this, then our model will learn to predict `[PAD]` tokens (yikes!). Therefore, we also need to create an _attention mask_ that tells us which tokens to ignore in the input because they are padding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a fake dataset using 26 aphabets, 1000 sentences, 10-20 words per sentence randomly\n",
    "data = [' '.join([chr(97 + i) for i in range(random.randint(10, 20))]) for _ in range(1000)]\n",
    "\n",
    "dataset = MLMDataset(tokenizer, data)\n",
    "dataset.tokenize()\n",
    "\n",
    "# get the first item\n",
    "input_ids, labels = dataset[0]\n",
    "print('input_ids:', input_ids)\n",
    "print('labels:', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch: List[Tuple[List[int], List[int]]]):\n",
    "    '''\n",
    "    A function that takes a list of instances in the dataset and collates them into a batch.\n",
    "    '''\n",
    "\n",
    "    ################################################################\n",
    "    #                    TODO: YOUR CODE HERE                      #\n",
    "    # 1. Separate the input_ids and the labels into two lists      #\n",
    "    input_ids_list = [torch.tensor(i[0]) for i in batch]\n",
    "    labels_list = [torch.tensor(i[1]) for i in batch]\n",
    "    # 2. Pad both lists using the tokenizer's padding value so \n",
    "    #    that all lists are the same length.\n",
    "    padded_input_ids = torch.nn.utils.rnn.pad_sequence(input_ids_list, batch_first=True, padding_value=0)\n",
    "    padded_labels = torch.nn.utils.rnn.pad_sequence(labels_list, batch_first=True, padding_value=-100)  # -100 is used for padding in MLM loss\n",
    "    \n",
    "    # 3. Create a boolean attention mask for the input_ids which specifies\n",
    "    #    which tokens are non-padded elements.\n",
    "    mask = (padded_input_ids != 0)\n",
    "\n",
    "    # 4. Return the padded input_ids, the attention mask, and the padded labels.\n",
    "    return padded_input_ids, mask, padded_labels\n",
    "    # NOTE: Look at nn.utils.rnn.pad_sequence\n",
    "    #\n",
    "    ################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the collate function\n",
    "batch_size = 8\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# All of the items in the batch should have the same shape!\n",
    "for input_ids, attention_mask, labels in dataloader:\n",
    "    print(input_ids.shape)\n",
    "    print(attention_mask.shape)\n",
    "    print(labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3.3: Train BERT for MLM!\n",
    "\n",
    "Once we have our `Dataset` and `collate` function, it's time to train the pre-model the BERT model for MLM. Working on your laptop to start, try training for 1-2 epochs on the `med` dataset. There's no guarantee the model will learn anything useful, but we can do some experiments later in Part 4 to take a look. \n",
    "\n",
    "The core steps for training are the following:\n",
    "1. Define your model\n",
    "2. Set up `wandb` for tracking its progress (this will be useful for monitoring when running on Great Lakes)\n",
    "3. Define the optimizers, learning rate, etc.\n",
    "4. Define the core training loop\n",
    "\n",
    "The code won't look too dissimilar from past pytorch training loops in Homeworks 1 and 2.\n",
    "\n",
    "Training times can vary, but on an M1 Mac and the default hyperparameters, one epoch takes ~50min on the large dataset with \"mps\" and 6 hours on \"cpu\". The model and training seem to fit at 12GB of memory. \n",
    "\n",
    "For training, we recommend trying to run either the medium or large for a few epochs. If your CPU is slower, try medium just for one epoch to get a sense of what it's learned in Part 4.  For getting a sense of whether the model is working, we strongly recommend testing your CPU-trained model with the masked language modeling task in Task 4.4 to see whether it can correctly fill in common words. The most similar words aren't always a good indicator that it's working.\n",
    "\n",
    "Don't worry if you can't train too long on your own machine. Once you've gotten the model working (e.g., some preliminary analysis in Part 4 looks \"okay\" -- doesn't have to be great), it's time to convert this to a script and run it on Great Lakes in Part 3.5 (more details there). You'll use the advanced GPUs on the cluster to go through 5 epochs (or more) during training, which will give you a good enough model that you can use it for the rest of the homework in Parts 4 and 5.\n",
    "\n",
    "\n",
    "#### VERY OPTIONAL PARTS:\n",
    "\n",
    "If you are feeling _really_ adventurous, you can try to speed up your model by trying a few of the much fancier things in pytorch. In practice, you should not need these for the homework. However, they can be fun to explore even with a CPU, though they make the biggest impact if you have access to a GPU too\n",
    "\n",
    "- Try using [torch.compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html) to optimize the `nn.Module` code (this tries to pre-compile the computation graph)\n",
    "- Rather than train with 32-bit floating point, use [amp](https://pytorch.org/blog/what-every-user-should-know-about-mixed-precision-training-in-pytorch/) to do mixed-precision training (i.e., fewer bits and faster). If you want to test this on Great Lakes, the GPUs there support \"fp16\" which will greatly speed up training. At the moment, `amp` isn't supported on \"mps\" devices, though it might show up [soon](https://github.com/pytorch/pytorch/issues/88415).\n",
    "- Implement an alternative training loop using [`accelerate`](https://github.com/huggingface/accelerate) and try using mixed precision (fp8, fp16, bf16) training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate with our real data!\n",
    "\n",
    "#review_data_path = './reviews-word2vec.med.txt' # <- for sanity checking / debugging\n",
    "#review_data_path = './reviews-word2vec.large.txt.gz' # <- for CPU pre-training and validating\n",
    "review_data_path = './reviews-word2vec.larger.txt.gz' #<- for GPU pre-training and validating (Part 3.5)\n",
    "\n",
    "\n",
    "# NOTE: when you eventually deploy this code to Great Lakes, you'll need to use the larger dataset \n",
    "# (see the PDF for notes/details)\n",
    "\n",
    "ofunc = gzip.open if review_data_path.endswith('gz') else open\n",
    "with ofunc(review_data_path, 'rt') as f:\n",
    "    reviews = f.readlines()\n",
    "    reviews = [review.strip() for review in reviews]\n",
    "\n",
    "dataset = MLMDataset(tokenizer, reviews)\n",
    "dataset.tokenize()\n",
    "\n",
    "# get the first item\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the collate function\n",
    "batch_size = 8\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# All of the items in the batch should have the same shape!\n",
    "for input_ids, attention_mask, labels in dataloader:\n",
    "    print(input_ids.shape)\n",
    "    print(attention_mask.shape)\n",
    "    print(labels.shape)\n",
    "    break\n",
    "\n",
    "#Should be torch.Size([8, 119]) for large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Now, lets put it all together in the training loop.\n",
    "'''\n",
    "\n",
    "# check if gpu is available\n",
    "device = 'cpu' \n",
    "if torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "print(f\"Using '{device}' device\")\n",
    "\n",
    "################################################################\n",
    "#         TODO: YOUR CODE HERE    \n",
    "#\n",
    "# 1. Define the loss function, optimizer, and the BERT model. Use the model hyperparameters from the PDF.\n",
    "model = BERT(vocab_size=tokenizer.get_vocab_size(),\n",
    "        feed_forward_size= 256,\n",
    "        hidden_size= 128,\n",
    "        num_layers= 2,\n",
    "        num_heads= 4)\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "f_loss = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "# 2. Specify the hyperparameters for training (e.g., learning rate, batch size, etc.)\n",
    "batch_size = 16\n",
    "num_epochs = 10\n",
    "\n",
    "# 3. Initialize wandb for logging.\n",
    "wandb.init(project='EECS595-BERT', name=f'experiment-{time.time()}')\n",
    "wandb.watch(model, log_freq=100)\n",
    "#\n",
    "################################################################\n",
    "\n",
    "\n",
    "losses = []\n",
    "print(\"Training\")\n",
    "################################################################\n",
    "#         TODO: YOUR CODE HERE       #\n",
    "# THe training loop is where we train the model.              #\n",
    "for epoch in trange(num_epochs, desc=\"Epoch\"):\n",
    "    #                                                             #\n",
    "    # You should implement the following:                         #\n",
    "    # 1. Load the input_ids, attention_mask, and labels to the    #\n",
    "    #    device.                                                  #\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    for step, (input_ids, attention_mask, labels) in enumerate(tqdm(dataloader, position=1, leave=True, desc=\"Step\")):\n",
    "        #send to device\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # 2. Zero the gradients of the optimizer.                     #\n",
    "        optimizer.zero_grad()\n",
    "        # 3. Get the output from the BERT model.                      #\n",
    "        outputs, attn = model(input_ids, attention_mask)\n",
    "        # 4. Calculate the loss using the output and the labels.      #\n",
    "        #labels = labels.unsqueeze(2).expand(outputs.shape[0], outputs.shape[1], outputs.shape[2])\n",
    "        outputs = outputs.view(-1, outputs.size(-1))\n",
    "        labels = labels.view(-1)\n",
    "        loss = f_loss(outputs, labels)\n",
    "        # 5. Backpropagate the loss.                                  #\n",
    "        loss.backward()\n",
    "        # 6. Update the optimizer.                                    #\n",
    "        optimizer.step()\n",
    "        # 7. Log the loss to wandb.                                   #\n",
    "        if (step % 100 == 0 and step != 0):\n",
    "            wandb.log({\"loss\": loss.item()})\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    # 8. Save the model every epoch. Use separate files per epoch #\n",
    "    torch.save(model.state_dict(), f\"bert8_{epoch}.sd\")\n",
    "#\n",
    "# HINT: The tokenizer can be helpful here.\n",
    "################################################################\n",
    "torch.save(model.state_dict(), f\"bert8_pretrained.sd\")\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Now that we have trained the model, we can use it to predict the masked tokens.\n",
    "'''\n",
    "\n",
    "# Predict the masked tokens\n",
    "def noise_inputs(inputs, mask_token_id, mlm_probability=0.15):\n",
    "    inputs = deepcopy(inputs)\n",
    "    labels = [-100] * len(inputs)\n",
    "    masked_indices = np.random.choice(len(inputs), int(len(inputs) * mlm_probability), replace=False)\n",
    "    for i in masked_indices:\n",
    "        if inputs[i] not in [mask_token_id, 101, 102, 0]:\n",
    "            inputs[i] = mask_token_id\n",
    "            labels[i] = inputs[i]\n",
    "    return inputs, labels\n",
    "\n",
    "def noise_and_predict_tokens(query, tokenizer, model) -> str:\n",
    "    with torch.no_grad():\n",
    "        tokenized_input = tokenizer.encode(query)        \n",
    "        tokens = tokenized_input.tokens\n",
    "        print('Original:', ' '.join(tokens))\n",
    "        ids = np.array(tokenized_input.ids)\n",
    "        inputs, labels = noise_inputs(ids, tokenizer.token_to_id('[MASK]'))\n",
    "        print('Noised: ', ' '.join([tokenizer.id_to_token(at_i) for at_i in inputs]))\n",
    "        \n",
    "        response, attns = model(torch.from_numpy(inputs).unsqueeze(0).to('cpu'))\n",
    "        response = response.argmax(-1).squeeze(0).tolist()\n",
    "        print('Guess:  ', ' '.join([tokenizer.id_to_token(at_i) for at_i in response[1:-1]]).replace(' ##', ''))\n",
    "\n",
    "s = 'I really like the book it was great and I loved reading it.'\n",
    "noise_and_predict_tokens(s, tokenizer, bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3.4: Save the Pre-Trained Model\n",
    "\n",
    "At this point, save the model's parameters in its `state_dict`. See pytorch's [documentation](https://pytorch.org/tutorials/beginner/saving_loading_models.html) for some guidance here. We'll be using this pre-trained model in later notebooks so once you save it, test that you can load it in another notebook (try `BERT_Inference.ipynb` to start) before moving on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "#                     TODO: YOUR CODE HERE                     #\n",
    "#\n",
    "# 1. Save the BERT model to a file\n",
    "torch.save(model.state_dict(), f\"bert8_pretrained.sd\")\n",
    "#\n",
    "# NOTE: Before you close this notebook, verify you can load the model and \n",
    "#       use it to predict the masked tokens in the BERT_Inference notebook.\n",
    "#\n",
    "################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3.5 Convert the notebook to a script and submit to Great Lakes for final training\n",
    "\n",
    "Once you have your model debugged and can verify that it works on a small dataset (manual exploration in Part 4 will help), it's time to train it on more data and for a longer time period. To do this, we'll use the Great Lakes cluster at U-M which will give you access to a GPU that will make training run ~10x faster; this means more epochs and more data in the same amount of time so you get a better model. The Homework PDF has documentation on how use Great Lakes if you haven't seen it. The course account is limited to 4 hours of wallclock time and 16GB of memory, which were tuned specific for this assignment.\n",
    "\n",
    "Great Lakes supports interactive mode with Jupyter and running a script as a job. We **strongly** encourage the latter. To get a GPU, you'll need to submit a job to the cluster, which uses [SLURM for scheduling](https://arc.umich.edu/greatlakes/slurm-user-guide/). If you attempt to queue for an interactive job, you will have no control over when it starts, so you may end up having your notebook run for 4 hours from 3am to 7am and then it ends, at which point you have to get back in the queue. If you submit a job as a script (i.e., a .py file that runs the code in this notebook), it will run for the specified amount of time and save the BERT model without you having to interact with anything. \n",
    "\n",
    "SLURM and cluster scheduling is very common in some industries where there is a single cluster resource and people share it by submitting jobs to run so that no one can monopolize the system and that jobs can run in parallel. Given that Great Lakes will be useful to you in future assignments and projects, we strongly encourage you to learn how to use it effectively in this assignment.\n",
    "\n",
    "Depending on how you're working on this file, there's a few ways to directly convert the notebook to a file if you use [Jupyter or the command line](https://mljar.com/blog/convert-jupyter-notebook-python/) or [VSCode](https://stackoverflow.com/questions/64297272/best-way-to-convert-ipynb-to-py-in-vscode). Once you convert it, you'll modify the file some to change the epochs and text file as specified in the PDF. **We also strongly recommend having your script save the model at the end of every epoch.**  That way, if your script takes longer than 4 hours and gets killed, you still have the best saved model you could get based on the amount of training you could do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyObT2o6Mn3FXBXu/mG1x7SL",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
